#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üéØ FULL DATA VERIFICATION TEST
‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏°‡∏ô‡∏π‡∏ó‡∏µ‡πà 1 ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 100% ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏Å‡∏±‡∏î

REQUIREMENTS:
- ‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• CSV ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÑ‡∏°‡πà‡∏°‡∏µ nrows, sample, chunk
- ‚úÖ ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ‡πÑ‡∏°‡πà‡∏°‡∏µ .head(), .iloc[]
- ‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ fallback, mock, dummy data ‡πÉ‡∏î‡πÜ
- ‚úÖ Full Power Mode ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
"""

import sys
import os
from pathlib import Path
import pandas as pd
import logging

# Add project root to path
sys.path.append(str(Path(__file__).parent))

# Import necessary modules
from core.project_paths import get_project_paths
from elliott_wave_modules.data_processor import ElliottWaveDataProcessor
from ultimate_full_power_config import ULTIMATE_FULL_POWER_CONFIG, apply_full_power_mode

def test_full_data_loading():
    """Test that all CSV data is loaded without limits"""
    print("üéØ TESTING FULL DATA LOADING - NO LIMITS")
    print("=" * 60)
    
    # Get project paths
    paths = get_project_paths()
    datacsv_path = paths.datacsv
    
    # Check CSV files
    csv_files = list(datacsv_path.glob("*.csv"))
    print(f"üìÅ Found CSV files: {len(csv_files)}")
    
    for csv_file in csv_files:
        print(f"  üìä {csv_file.name}")
        
        # Load with pandas directly to get full size
        df_full = pd.read_csv(csv_file)
        full_rows = len(df_full)
        print(f"    Full size: {full_rows:,} rows")
        
        # Test data processor loading
        config = apply_full_power_mode({})
        processor = ElliottWaveDataProcessor(config=config)
        
        # Load data through processor
        df_processed = processor.load_real_data()
        
        if df_processed is not None:
            processed_rows = len(df_processed)
            print(f"    Processed size: {processed_rows:,} rows")
            
            # Check if we're using all data
            if processed_rows >= full_rows * 0.95:  # Allow for minimal cleaning
                print(f"    ‚úÖ FULL DATA USED: {processed_rows:,}/{full_rows:,} ({processed_rows/full_rows*100:.1f}%)")
            else:
                print(f"    ‚ùå DATA REDUCED: {processed_rows:,}/{full_rows:,} ({processed_rows/full_rows*100:.1f}%)")
                return False
        else:
            print(f"    ‚ùå FAILED TO LOAD DATA")
            return False
    
    return True

def test_feature_engineering():
    """Test that feature engineering uses all data"""
    print("\nüß† TESTING FEATURE ENGINEERING - NO LIMITS")
    print("=" * 60)
    
    # Apply full power config
    config = apply_full_power_mode({})
    processor = ElliottWaveDataProcessor(config=config)
    
    # Load data
    data = processor.load_real_data()
    if data is None:
        print("‚ùå Failed to load data")
        return False
    
    initial_rows = len(data)
    print(f"üìä Initial data: {initial_rows:,} rows")
    
    # Create features
    features = processor.create_elliott_wave_features(data)
    feature_rows = len(features)
    
    print(f"üéØ Features created: {feature_rows:,} rows")
    print(f"üìà Features count: {features.shape[1]} columns")
    
    # Check data preservation
    data_preservation = feature_rows / initial_rows * 100
    print(f"üíæ Data preservation: {data_preservation:.1f}%")
    
    if data_preservation >= 95.0:
        print("‚úÖ EXCELLENT: >95% data preserved")
        return True
    elif data_preservation >= 90.0:
        print("‚ö†Ô∏è GOOD: >90% data preserved")
        return True
    else:
        print("‚ùå POOR: <90% data preserved")
        return False

def test_ml_preparation():
    """Test that ML data preparation uses all available data"""
    print("\nü§ñ TESTING ML DATA PREPARATION - NO LIMITS")
    print("=" * 60)
    
    config = apply_full_power_mode({})
    processor = ElliottWaveDataProcessor(config=config)
    
    # Load and process
    data = processor.load_real_data()
    features = processor.create_elliott_wave_features(data)
    initial_rows = len(features)
    
    # Prepare ML data
    X, y = processor.prepare_ml_data(features)
    ml_rows = len(X)
    
    print(f"üìä Features data: {initial_rows:,} rows")
    print(f"üéØ ML data: {ml_rows:,} rows")
    print(f"üìà Features count: {X.shape[1]} columns")
    print(f"‚öñÔ∏è Target balance: {y.mean():.3f}")
    
    # Check data usage
    data_usage = ml_rows / initial_rows * 100
    print(f"üíØ Data usage: {data_usage:.1f}%")
    
    if data_usage >= 90.0:
        print("‚úÖ EXCELLENT: >90% data used for ML")
        return True
    else:
        print("‚ùå INSUFFICIENT: <90% data used for ML")
        return False

def verify_no_limits_config():
    """Verify that full power config is applied correctly"""
    print("\n‚öôÔ∏è VERIFYING FULL POWER CONFIGURATION")
    print("=" * 60)
    
    config = apply_full_power_mode({})
    
    # Check data processor config
    data_config = config.get('data_processor', {})
    print(f"üìä load_all_data: {data_config.get('load_all_data', False)}")
    print(f"üö´ sampling_disabled: {data_config.get('sampling_disabled', False)}")
    print(f"üíæ chunk_size: {data_config.get('chunk_size', 'default')}")
    
    # Check feature selector config
    feature_config = config.get('feature_selector', {})
    print(f"üéØ target_auc: {feature_config.get('target_auc', 'default')}")
    print(f"üî¢ max_features: {feature_config.get('max_features', 'default')}")
    print(f"‚è±Ô∏è timeout_minutes: {feature_config.get('timeout_minutes', 'default')}")
    
    # Validate settings
    checks = [
        data_config.get('load_all_data', False),
        data_config.get('sampling_disabled', False),
        data_config.get('chunk_size', 1) == 0,  # 0 means no chunking
        feature_config.get('timeout_minutes', 1) == 0,  # 0 means no timeout
        feature_config.get('max_features', 0) >= 50,  # At least 50 features
    ]
    
    passed = sum(checks)
    total = len(checks)
    
    print(f"\nüîç Configuration checks: {passed}/{total} passed")
    
    if passed == total:
        print("‚úÖ FULL POWER CONFIG VERIFIED")
        return True
    else:
        print("‚ùå CONFIGURATION ISSUES DETECTED")
        return False

def main():
    """Run all verification tests"""
    print("üöÄ FULL DATA VERIFICATION TEST SUITE")
    print("=" * 80)
    print("OBJECTIVE: Verify Menu 1 uses 100% of CSV data with NO limits")
    print("=" * 80)
    
    tests = [
        ("Full Data Loading", test_full_data_loading),
        ("Feature Engineering", test_feature_engineering),
        ("ML Data Preparation", test_ml_preparation),
        ("Full Power Config", verify_no_limits_config),
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"‚ùå {test_name} FAILED: {e}")
            results.append((test_name, False))
    
    # Summary
    print("\n" + "=" * 80)
    print("üìä TEST SUMMARY")
    print("=" * 80)
    
    passed = 0
    for test_name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status} {test_name}")
        if result:
            passed += 1
    
    total = len(results)
    success_rate = passed / total * 100
    
    print(f"\nüéØ OVERALL RESULT: {passed}/{total} tests passed ({success_rate:.1f}%)")
    
    if success_rate == 100:
        print("üèÜ PERFECT: Menu 1 uses 100% data with NO limits!")
        print("‚úÖ READY FOR ENTERPRISE PRODUCTION")
    elif success_rate >= 75:
        print("‚ö†Ô∏è GOOD: Most tests passed, minor issues may exist")
    else:
        print("‚ùå CRITICAL: Major issues detected, requires fixes")
    
    return success_rate == 100

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
