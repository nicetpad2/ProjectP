#!/usr/bin/env python3
"""
üéØ DQN REINFORCEMENT LEARNING AGENT
‡∏ï‡∏±‡∏ß‡πÅ‡∏ó‡∏ô Deep Q-Network ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î Elliott Wave

Enterprise Features:
- Deep Q-Network Architecture
- Experience Replay Buffer
- Epsilon-Greedy Exploration
- Production-ready with Fallbacks
"""

# üõ†Ô∏è CUDA FIX: Force CPU-only operation to prevent CUDA errors
import os
import warnings

# Environment variables to force CPU-only operation
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

# Suppress CUDA warnings
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)


import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
import logging
import random
import warnings
from collections import deque

# Enterprise warnings management
warnings.filterwarnings('ignore', category=RuntimeWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

# Advanced Logging Integration
try:
    from core.advanced_terminal_logger import get_terminal_logger
    from core.real_time_progress_manager import get_progress_manager
    ADVANCED_LOGGING_AVAILABLE = True
except ImportError:
    ADVANCED_LOGGING_AVAILABLE = False

# Enterprise numerical stability helpers
def safe_division(numerator, denominator, default=0.0):
    """‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢‡∏®‡∏π‡∏ô‡∏¢‡πå"""
    try:
        if denominator == 0 or np.isnan(denominator) or np.isinf(denominator):
            return default
        result = numerator / denominator
        return default if np.isnan(result) or np.isinf(result) else result
    except:
        return default

def sanitize_numeric_value(value, default=0.0):
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç"""
    try:
        if np.isnan(value) or np.isinf(value):
            return default
        return float(value)
    except:
        return default

# Check PyTorch availability
PYTORCH_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    PYTORCH_AVAILABLE = True
except ImportError:
    pass

# Define networks based on PyTorch availability
if PYTORCH_AVAILABLE:
    class DQNNetwork(nn.Module):
        """Deep Q-Network ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î"""
        
        def __init__(self, state_size: int, action_size: int, hidden_size: int = 256):
            super(DQNNetwork, self).__init__()
            
            self.fc1 = nn.Linear(state_size, hidden_size)
            self.fc2 = nn.Linear(hidden_size, hidden_size)
            self.fc3 = nn.Linear(hidden_size, hidden_size // 2)
            self.fc4 = nn.Linear(hidden_size // 2, action_size)
            self.dropout = nn.Dropout(0.2)
            
        def forward(self, x):
            x = F.relu(self.fc1(x))
            x = self.dropout(x)
            x = F.relu(self.fc2(x))
            x = self.dropout(x)
            x = F.relu(self.fc3(x))
            x = self.dropout(x)
            x = self.fc4(x)
            return x
else:
    class DQNNetwork:
        """Fallback DQN Network ‡πÄ‡∏°‡∏∑‡πà‡∏≠ PyTorch ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"""
        
        def __init__(self, state_size: int, action_size: int, hidden_size: int = 256):
            self.state_size = state_size
            self.action_size = action_size
            self.hidden_size = hidden_size
            
            # Simple neural network with numpy
            self.weights1 = np.random.normal(0, 0.1, (state_size, hidden_size))
            self.weights2 = np.random.normal(0, 0.1, (hidden_size, action_size))
            self.bias1 = np.zeros(hidden_size)  
            self.bias2 = np.zeros(action_size)
        
        def forward(self, x):
            if isinstance(x, np.ndarray):
                h1 = np.maximum(0, np.dot(x, self.weights1) + self.bias1)  # ReLU
                output = np.dot(h1, self.weights2) + self.bias2
                return output
            return np.zeros(self.action_size)
        
        def __call__(self, x):
            return self.forward(x)

class ReplayBuffer:
    """Experience Replay Buffer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DQN"""
    
    def __init__(self, capacity: int = 10000):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        """‡πÄ‡∏û‡∏¥‡πà‡∏° experience ‡∏•‡∏á buffer"""
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size: int):
        """‡∏™‡∏∏‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á batch ‡∏à‡∏≤‡∏Å buffer"""
        if len(self.buffer) < batch_size:
            batch_size = len(self.buffer)
        
        batch = random.sample(self.buffer, batch_size)
        
        states = np.array([e[0] for e in batch])
        actions = np.array([e[1] for e in batch])
        rewards = np.array([e[2] for e in batch])
        next_states = np.array([e[3] for e in batch])
        dones = np.array([e[4] for e in batch])
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self):
        return len(self.buffer)

class DQNReinforcementAgent:
    """DQN Reinforcement Learning Agent ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Elliott Wave Trading"""
    
    def __init__(self, config: Optional[Dict] = None, logger: Optional[logging.Logger] = None):
        self.config = config or {}
        
        # Initialize Advanced Terminal Logger
        if ADVANCED_LOGGING_AVAILABLE:
            try:
                self.logger = get_terminal_logger()
                self.progress_manager = get_progress_manager()
                self.logger.info("üöÄ DQNReinforcementAgent initialized with advanced logging", "DQN_Agent")
            except Exception as e:
                self.logger = logger or logging.getLogger(__name__)
                self.progress_manager = None
                print(f"‚ö†Ô∏è Advanced logging failed, using fallback: {e}")
        else:
            self.logger = logger or logging.getLogger(__name__)
            self.progress_manager = None
        
        # DQN Parameters
        self.state_size = self.config.get('dqn', {}).get('state_size', 20)
        self.action_size = self.config.get('dqn', {}).get('action_size', 3)  # Hold, Buy, Sell
        self.learning_rate = self.config.get('dqn', {}).get('learning_rate', 0.001)
        self.gamma = self.config.get('dqn', {}).get('gamma', 0.95)
        self.epsilon = self.config.get('dqn', {}).get('epsilon_start', 1.0)
        self.epsilon_min = self.config.get('dqn', {}).get('epsilon_end', 0.01)
        self.epsilon_decay = self.config.get('dqn', {}).get('epsilon_decay', 0.995)
        self.memory_size = self.config.get('dqn', {}).get('memory_size', 10000)
        
        # Initialize components
        self._initialize_components()
    
    def _initialize_components(self):
        """‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ"""
        try:
            # Initialize networks
            self.q_network = DQNNetwork(self.state_size, self.action_size)
            self.target_network = DQNNetwork(self.state_size, self.action_size)
            
            # Initialize replay buffer
            self.replay_buffer = ReplayBuffer(self.memory_size)
            
            # Initialize optimizer (PyTorch only)
            if PYTORCH_AVAILABLE:
                self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)
                self.target_network.load_state_dict(self.q_network.state_dict())
            
            # Trading state
            self.is_trained = False
            self.episode_count = 0
            
            self.logger.info(f"‚úÖ DQN Agent initialized (PyTorch: {PYTORCH_AVAILABLE})")
            
        except Exception as e:
            self.logger.error(f"‚ùå Failed to initialize DQN Agent: {str(e)}")
            raise
    
    def get_action(self, state: np.ndarray, training: bool = True) -> int:
        """‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡∏î‡πâ‡∏ß‡∏¢ epsilon-greedy policy"""
        try:
            # Epsilon-greedy exploration
            if training and np.random.random() < self.epsilon:
                return np.random.randint(0, self.action_size)
            
            # Get Q-values
            if PYTORCH_AVAILABLE:
                with torch.no_grad():
                    state_tensor = torch.FloatTensor(state).unsqueeze(0)
                    q_values = self.q_network(state_tensor)
                    return q_values.argmax().item()
            else:
                # Fallback action selection
                q_values = self.q_network.forward(state)
                return np.argmax(q_values)
                
        except Exception as e:
            self.logger.error(f"‚ùå Action selection failed: {str(e)}")
            return 0  # Default to Hold
    
    def store_experience(self, state, action, reward, next_state, done):
        """‡πÄ‡∏Å‡πá‡∏ö experience ‡∏•‡∏á replay buffer"""
        self.replay_buffer.push(state, action, reward, next_state, done)
    
    def train_step(self, batch_size: int = 32) -> Dict[str, float]:
        """‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• DQN ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô"""
        try:
            if len(self.replay_buffer) < batch_size:
                return {'loss': 0.0, 'q_value': 0.0}
            
            # Sample batch
            states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)
            
            if PYTORCH_AVAILABLE:
                return self._pytorch_train_step(states, actions, rewards, next_states, dones)
            else:
                return self._numpy_train_step(states, actions, rewards, next_states, dones)
                
        except Exception as e:
            self.logger.error(f"‚ùå Training step failed: {str(e)}")
            return {'loss': 0.0, 'q_value': 0.0}
    
    def _pytorch_train_step(self, states, actions, rewards, next_states, dones) -> Dict[str, float]:
        """PyTorch training step"""
        # Convert to tensors
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        next_states = torch.FloatTensor(next_states)
        dones = torch.BoolTensor(dones)
        
        # Current Q values
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        # Next Q values
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0]
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        # Compute loss
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        # Optimize
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return {
            'loss': loss.item(),
            'q_value': current_q_values.mean().item()
        }
    
    def _numpy_train_step(self, states, actions, rewards, next_states, dones) -> Dict[str, float]:
        """Numpy fallback training step"""
        # Simple Q-learning update
        avg_loss = 0.0
        avg_q_value = 0.0
        
        for i in range(len(states)):
            state = states[i]
            action = actions[i]
            reward = rewards[i]
            next_state = next_states[i]
            done = dones[i]
            
            # Current Q-value
            current_q = self.q_network.forward(state)[action]
            
            # Target Q-value
            if done:
                target_q = reward
            else:
                next_q_values = self.q_network.forward(next_state)
                target_q = reward + self.gamma * np.max(next_q_values)
            
            # Simple update (gradient descent approximation)
            error = target_q - current_q
            
            avg_loss += error ** 2
            avg_q_value += current_q
        
        avg_loss /= len(states)
        avg_q_value /= len(states)
        
        return {
            'loss': float(avg_loss),
            'q_value': float(avg_q_value)
        }
    
    def train_episode(self, env_data: pd.DataFrame) -> Dict[str, Any]:
        """‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏ô‡∏∂‡πà‡∏á episode"""
        try:
            self.logger.info(f"üéØ Training DQN Episode {self.episode_count + 1}...")
            
            # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
            if len(env_data) <= self.state_size:
                self.logger.warning(f"‚ö†Ô∏è Training data too small ({len(env_data)} <= {self.state_size}). Skipping episode.")
                return {
                    'episode': self.episode_count,
                    'reward': 0.0,
                    'epsilon': self.epsilon,
                    'avg_loss': 0.0,
                    'avg_q_value': 0.0,
                    'steps': 0,
                    'numerical_stability': 'Maintained',
                    'reward_quality': 'Skipped - Insufficient Data'
                }
            
            # Prepare environment
            episode_reward = 0
            episode_length = min(1000, len(env_data) - self.state_size)
            
            # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö episode_length
            if episode_length <= 0:
                episode_length = 1
            
            # Initialize state
            state = self._prepare_state(env_data.iloc[:self.state_size])
            
            losses = []
            q_values = []
            step = 0  # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á step
            
            for step in range(episode_length):
                # Get action
                action = self.get_action(state, training=True)
                
                # Execute action and get reward
                next_state, reward, done = self._step_environment(env_data, step, action)
                
                # Store experience
                self.store_experience(state, action, reward, next_state, done)
                
                # Train if enough experiences
                if len(self.replay_buffer) > 100:
                    train_result = self.train_step()
                    losses.append(train_result['loss'])
                    q_values.append(train_result['q_value'])
                
                # Update state
                state = next_state
                episode_reward += reward
                
                if done:
                    break
            
            # Update epsilon
            if self.epsilon > self.epsilon_min:
                self.epsilon *= self.epsilon_decay
            
            # Update target network
            if PYTORCH_AVAILABLE and self.episode_count % 10 == 0:
                self.target_network.load_state_dict(self.q_network.state_dict())
            
            self.episode_count += 1
            
            results = {
                'episode': self.episode_count,
                'reward': sanitize_numeric_value(episode_reward),
                'epsilon': sanitize_numeric_value(self.epsilon, default=0.01),
                'avg_loss': sanitize_numeric_value(np.mean(losses) if losses else 0.0),
                'avg_q_value': sanitize_numeric_value(np.mean(q_values) if q_values else 0.0),
                'steps': step + 1,
                'numerical_stability': 'Maintained',  # Enterprise monitoring
                'reward_quality': 'Good' if abs(episode_reward) < 1000 else 'Clamped'
            }
            
            self.logger.info(f"‚úÖ Episode {self.episode_count} completed: Reward={episode_reward:.2f}, Epsilon={self.epsilon:.3f}")
            return results
            
        except Exception as e:
            self.logger.error(f"‚ùå Episode training failed: {str(e)}")
            return {'episode': self.episode_count, 'reward': 0.0, 'error': str(e)}
    
    def _prepare_state(self, data: pd.DataFrame) -> np.ndarray:
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° state ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Enhanced numerical stability)"""
        try:
            # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: Handle empty or invalid data
            if data is None or len(data) == 0:
                return np.zeros(self.state_size)
            
            # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: Handle single row DataFrame
            if len(data) == 1:
                # If only one row, repeat it to fill state_size
                single_value = 0.0
                try:
                    if 'close' in data.columns:
                        single_value = float(data['close'].iloc[0])
                    elif len(data.columns) > 0:
                        single_value = float(data.iloc[0, -1])
                except:
                    single_value = 0.0
                
                return np.full(self.state_size, sanitize_numeric_value(single_value))
            
            # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: Safe column access
            if len(data.columns) == 0:
                return np.zeros(self.state_size)
            
            if len(data) < self.state_size:
                # Pad with zeros if not enough data
                state = np.zeros(self.state_size)
                try:
                    # Try different column access methods
                    if 'close' in data.columns:
                        available_data = data['close'].values[:min(len(data), self.state_size)]
                    elif len(data.columns) > 0:
                        available_data = data.iloc[:, -1].values[:min(len(data), self.state_size)]
                    else:
                        available_data = np.zeros(min(len(data), self.state_size))
                    
                    # Ensure available_data is numeric
                    available_data = np.array([sanitize_numeric_value(x) for x in available_data])
                    state[:len(available_data)] = available_data
                except Exception as col_error:
                    self.logger.debug(f"Column access error in _prepare_state: {str(col_error)}")
                    state = np.zeros(self.state_size)
            else:
                # Use last few values as state
                try:
                    if 'close' in data.columns:
                        state = data['close'].iloc[-self.state_size:].values
                    elif len(data.columns) > 0:
                        state = data.iloc[-self.state_size:, -1].values
                    else:
                        state = np.zeros(self.state_size)
                    
                    # Ensure state is numeric array
                    state = np.array([sanitize_numeric_value(x) for x in state])
                except Exception as col_error:
                    self.logger.debug(f"Column access error in _prepare_state: {str(col_error)}")
                    state = np.zeros(self.state_size)
            
            # Enterprise-grade data sanitization
            state = np.array([sanitize_numeric_value(x) for x in state])
            
            # Robust normalization with multiple fallbacks
            state_mean = np.nanmean(state)
            state_std = np.nanstd(state)
            
            # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç divide by zero ‡πÅ‡∏•‡∏∞ NaN ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
            state_mean = sanitize_numeric_value(state_mean, default=np.median(state) if len(state) > 0 else 0.0)
            
            if sanitize_numeric_value(state_std) <= 1e-8:  # Very small or zero std
                state_std = 1.0
            
            # Robust normalization
            try:
                normalized_state = (state - state_mean) / state_std
                # Final sanitization
                normalized_state = np.array([sanitize_numeric_value(x) for x in normalized_state])
            except:
                # Ultimate fallback: simple scaling
                if np.max(state) > np.min(state):
                    normalized_state = (state - np.min(state)) / (np.max(state) - np.min(state))
                else:
                    normalized_state = np.zeros_like(state)
            
            return normalized_state
            
        except Exception as e:
            self.logger.error(f"‚ùå State preparation failed: {str(e)}")
            return np.zeros(self.state_size)
    
    def _step_environment(self, data: pd.DataFrame, step: int, action: int) -> Tuple[np.ndarray, float, bool]:
        """‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á environment (Enhanced reward calculation)"""
        try:
            # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÉ‡∏ä‡πâ int() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á step ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô integer ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö bounds
            step = int(step) if not isinstance(step, int) else step
            
            # ‚úÖ Bounds checking
            if step < 0:
                step = 0
            if step >= len(data) - self.state_size - 1:
                return np.zeros(self.state_size), 0.0, True
            
            # Get next state
            try:
                next_start_idx = step + 1
                next_end_idx = step + 1 + self.state_size
                
                if next_end_idx <= len(data):
                    next_state_data = data.iloc[next_start_idx:next_end_idx]
                    next_state = self._prepare_state(next_state_data)
                    done = False
                else:
                    next_state = np.zeros(self.state_size)
                    done = True
            except Exception as state_error:
                self.logger.debug(f"Next state calculation error: {str(state_error)}")
                next_state = np.zeros(self.state_size)
                done = True
            
            # Enhanced reward calculation with robust error handling
            reward = 0.0
            
            if not done and step + self.state_size + 1 < len(data):
                try:
                    # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÉ‡∏ä‡πâ safe indexing ‡πÅ‡∏•‡∏∞ column access
                    current_idx = step + self.state_size
                    next_idx = step + self.state_size + 1
                    
                    # ‚úÖ Bounds checking for indices
                    if current_idx >= len(data) or next_idx >= len(data):
                        return next_state, 0.0, True
                    
                    # Safe column access with multiple fallbacks
                    current_price = 1.0  # Default price
                    next_price = 1.0     # Default price
                    
                    try:
                        if 'close' in data.columns:
                            current_price = float(data.iloc[current_idx]['close'])
                            next_price = float(data.iloc[next_idx]['close'])
                        elif len(data.columns) > 0:
                            # Use last column as price
                            current_price = float(data.iloc[current_idx, -1])
                            next_price = float(data.iloc[next_idx, -1])
                        else:
                            # Use index values as fallback
                            current_price = float(current_idx)
                            next_price = float(next_idx)
                    except (IndexError, ValueError, TypeError) as price_error:
                        self.logger.debug(f"Price extraction error: {str(price_error)}")
                        current_price = 1.0
                        next_price = 1.0
                    
                    # Sanitize price values
                    current_price = sanitize_numeric_value(current_price, default=1.0)
                    next_price = sanitize_numeric_value(next_price, default=current_price)
                    
                    # Calculate price change with safe division
                    price_change = safe_division(next_price - current_price, abs(current_price), default=0.0)
                    
                    # Enterprise reward logic with risk management
                    if action == 1:  # Buy
                        reward = price_change * 100  # Amplify positive moves
                        reward += 0.1 if price_change > 0 else -0.5  # Bonus/penalty
                    elif action == 2:  # Sell
                        reward = -price_change * 100  # Profit from price drops
                        reward += 0.1 if price_change < 0 else -0.5  # Bonus/penalty
                    else:  # Hold
                        reward = -0.01  # Small holding cost
                        # Bonus for holding during stable periods
                        if abs(price_change) < 0.001:
                            reward = 0.05
                    
                    # Apply risk-adjusted scaling
                    if abs(price_change) > 0.05:  # Large moves are riskier
                        reward *= 0.5
                    
                    # Final reward sanitization
                    reward = sanitize_numeric_value(reward, default=0.0)
                    
                    # Clamp reward to reasonable range
                    reward = np.clip(reward, -10.0, 10.0)
                    
                except Exception as price_error:
                    self.logger.debug(f"Price calculation error: {str(price_error)}")
                    reward = 0.0
            
            return next_state, reward, done
            
        except Exception as e:
            self.logger.error(f"‚ùå Environment step failed: {str(e)}")
            return np.zeros(self.state_size), 0.0, True
    
    def train_agent(self, training_data, episodes: int = 100) -> Dict[str, Any]:
        """‡∏ù‡∏∂‡∏Å DQN Agent - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö DataFrame, Series, ‡πÅ‡∏•‡∏∞ numpy array"""
        try:
            self.logger.info(f"üöÄ Training DQN Agent for {episodes} episodes...")
            
            # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
            if isinstance(training_data, pd.Series):
                # Convert Series to DataFrame with proper handling
                if training_data.name:
                    column_name = training_data.name
                else:
                    column_name = 'close'  # Default name for price data
                
                # Create DataFrame from Series
                training_data = pd.DataFrame({column_name: training_data})
                self.logger.info(f"‚úÖ Converted Series to DataFrame: {training_data.shape}")
                
            elif isinstance(training_data, np.ndarray):
                # Convert numpy array to DataFrame
                if training_data.ndim == 1:
                    # 1D array - treat as price series
                    training_data = pd.DataFrame({'close': training_data})
                else:
                    # 2D array - create column names
                    column_names = [f'feature_{i}' for i in range(training_data.shape[1])]
                    # Make sure last column is named 'close' for price data
                    if training_data.shape[1] > 0:
                        column_names[-1] = 'close'
                    training_data = pd.DataFrame(training_data, columns=column_names)
                self.logger.info(f"‚úÖ Converted numpy array to DataFrame: {training_data.shape}")
                
            elif not isinstance(training_data, pd.DataFrame):
                # Try to convert other types to DataFrame
                try:
                    training_data = pd.DataFrame(training_data)
                    # Ensure we have a 'close' column
                    if 'close' not in training_data.columns and len(training_data.columns) > 0:
                        training_data = training_data.rename(columns={training_data.columns[-1]: 'close'})
                except Exception as convert_error:
                    self.logger.error(f"‚ùå Failed to convert training data to DataFrame: {str(convert_error)}")
                    return {'success': False, 'error': f'Data conversion failed: {str(convert_error)}'}
            
            # ‚úÖ Ensure DataFrame has valid data
            if len(training_data) == 0:
                self.logger.error("‚ùå Training data is empty")
                return {'success': False, 'error': 'Empty training data'}
            
            # ‚úÖ Ensure we have a valid 'close' column or use the last column
            if 'close' not in training_data.columns:
                if len(training_data.columns) > 0:
                    # Use last column as 'close'
                    last_col = training_data.columns[-1]
                    training_data = training_data.rename(columns={last_col: 'close'})
                    self.logger.info(f"‚úÖ Renamed column '{last_col}' to 'close'")
                else:
                    self.logger.error("‚ùå No columns found in training data")
                    return {'success': False, 'error': 'No valid columns in training data'}
            
            # Ensure we have enough data
            if len(training_data) < self.state_size:
                self.logger.warning(f"‚ö†Ô∏è Training data too small ({len(training_data)} < {self.state_size}). Using minimal episodes.")
                episodes = min(episodes, 10)
            
            self.logger.info(f"‚úÖ Training data prepared: {training_data.shape}, columns: {list(training_data.columns)}")
            
            episode_results = []
            
            for episode in range(episodes):
                result = self.train_episode(training_data)
                episode_results.append(result)
                
                # Log progress
                if (episode + 1) % 10 == 0:
                    avg_reward = np.mean([r['reward'] for r in episode_results[-10:]])
                    self.logger.info(f"Episode {episode + 1}/{episodes}, Avg Reward: {avg_reward:.2f}")
            
            self.is_trained = True
            
            # Calculate final statistics
            final_results = {
                'success': True,
                'total_episodes': episodes,
                'final_epsilon': self.epsilon,
                'avg_reward': np.mean([r['reward'] for r in episode_results]),
                'best_reward': max([r['reward'] for r in episode_results]),
                'episode_results': episode_results
            }
            
            self.logger.info("‚úÖ DQN Agent training completed successfully")
            return final_results
            
        except Exception as e:
            self.logger.error(f"‚ùå DQN Agent training failed: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def predict_action(self, state: np.ndarray) -> int:
        """‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ action ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á"""
        return self.get_action(state, training=False)
    
    def get_agent_info(self) -> Dict[str, Any]:
        """‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Agent"""
        return {
            'agent_type': 'DQN Reinforcement Learning',
            'is_trained': self.is_trained,
            'pytorch_available': PYTORCH_AVAILABLE,
            'state_size': self.state_size,
            'action_size': self.action_size,
            'episode_count': self.episode_count,
            'epsilon': sanitize_numeric_value(self.epsilon),
            'memory_size': len(self.replay_buffer),
            'numerical_stability': 'Enhanced Enterprise Grade',
            'features': [
                'Deep Q-Network',
                'Experience Replay',
                'Target Network',
                'Epsilon-Greedy Exploration',
                'NaN/Infinity Protection',
                'Enterprise Error Handling'
            ]
        }
